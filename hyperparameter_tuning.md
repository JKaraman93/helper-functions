### Number of Layers

ðŸ‘‰ When to increase layers?

    Complex tasks (e.g., image recognition, NLP, speech processing) â†’ Deep networks capture hierarchical features.

    High-dimensional input data (e.g., images, text embeddings) â†’ More layers help extract meaningful representations.

ðŸ‘‰ When to decrease layers?

    Simple tasks (e.g., linear classification) â†’ Too many layers lead to overfitting.

    Small datasets â†’ Deep networks memorize rather than generalize.

ðŸ”§ Tuning strategy:

    Start with 2-3 layers, increase gradually while monitoring validation loss.

    Use GridSearch or Bayesian Optimization to test different architectures.

### Number of Neurons per Layer

ðŸ‘‰ When to increase neurons?

    Complex patterns in data â†’ More neurons can capture richer representations.

    Underfitting (high bias, low accuracy) â†’ The model lacks capacity.

ðŸ‘‰ When to decrease neurons?

    Overfitting (high variance, training accuracy â‰« validation accuracy) â†’ Too many neurons memorize noise.

    Latency-sensitive applications â†’ Fewer neurons reduce computation time.

ðŸ”§ Tuning strategy:

    Start with (input features Ã— 2) neurons in the first layer.

    Try powers of 2 (e.g., 32, 64, 128, 256) and monitor performance.

    Use pruning (reduce neurons layer by layer if overfitting occurs).

### Learning Rate (LR)

ðŸ‘‰ When to increase LR?

    Training is too slow â†’ If loss decreases too gradually, increasing LR speeds up convergence.

    Large datasets â†’ Larger LR is effective since noise cancels out bad updates.

ðŸ‘‰ When to decrease LR?

    Diverging loss (spikes up and down) â†’ LR is too high, making updates unstable.

    Complex models (deep networks) â†’ Smaller LR prevents skipping local minima.

ðŸ”§ Tuning strategy:

    Start with 0.01 for SGD, 0.001 for Adam.

    Use LR scheduling: Decrease LR after plateaus.

    Use Cyclical LR or One-Cycle Policy for efficient training.

### Optimizer Choice

ðŸ‘‰ SGD (with momentum)

    Good for large datasets (e.g., ImageNet training).

    Can get stuck in local minima.

    Works well with a well-tuned LR schedule.

ðŸ‘‰ Adam

    Good for small datasets and NLP (handles noisy gradients well).

    Sometimes generalizes poorly compared to SGD.

ðŸ‘‰ RMSprop

    Works well in recurrent networks (RNNs, LSTMs).

    Effective for unstable loss surfaces.

ðŸ”§ Tuning strategy:

    Start with Adam for most cases, SGD with momentum for large datasets.

    Experiment with learning rate decay.

### Batch Size

ðŸ‘‰ When to increase batch size?

    Faster training â†’ Larger batches improve GPU utilization.

    BatchNorm present â†’ Works better with large batch sizes.

ðŸ‘‰ When to decrease batch size?

    Limited memory (large models, images) â†’ Smaller batches avoid OOM errors.

    Generalization issues â†’ Small batches introduce noise, helping models escape sharp local minima.

ðŸ”§ Tuning strategy:

    Use 32 or 64 as a starting point.

    Large datasets: 128, 256, 512.

    Small datasets: 8, 16, 32.

### Activation Function

ðŸ‘‰ ReLU

    Best for deep networks â†’ Avoids vanishing gradient.

    Can suffer from dead neurons (dying ReLU problem).

ðŸ‘‰ LeakyReLU / Parametric ReLU

    Fixes dead ReLU problem.

    Used in GANs and deeper models.

ðŸ‘‰ Tanh

    Works better for small networks.

    Can still suffer from vanishing gradients.

ðŸ‘‰ Sigmoid

    Only for binary classification.

    Avoid in deep networks (vanishing gradients).

ðŸ”§ Tuning strategy:

    ReLU is the default for deep networks.

    LeakyReLU or ELU can be used for better performance.

    Sigmoid/Tanh only for small networks or output layers.

### Hyperparameter Tuning Techniques

    GridSearchCV â€“ Works for shallow networks but too slow for deep learning.

    RandomizedSearchCV â€“ Faster, but not always optimal.

    Bayesian Optimization â€“ More efficient exploration.

    Hyperband / HalvingGridSearchCV â€“ Stops bad configurations early.

    Neural Architecture Search (NAS) â€“ Automates deep learning tuning.

### Final Recommendations
- Start simple (few layers, moderate neurons, Adam optimizer).
- Tune one hyperparameter at a time.
- Use callbacks (e.g., ReduceLROnPlateau, EarlyStopping).
- onsider AutoML tools if available (e.g., KerasTuner, Optuna).

### A Good Configuration

Hyperparameter | Default value 
 --- | --- 
|Kernel initializer | He initialization |
Activation function | ReLU if shallow; Swish if deep
Normalization | None if shallow; Batch Norm if deep
Regularization | Early stopping; Weight decay if needed
Optimizer | Nesterov Accelerated Gradients or AdamW
Learning rate schedule | Performance scheduling or 1cycle

<ins> In case of  simple stack of dense layers </ins>

Hyperparameter | Default value
 --- | --- 
Kernel initializer |LeCun initialization
Activation function | SELU
Normalization | None (self-normalization)
Regularization | Alpha dropout if needed
Optimizer |Nesterov Accelerated Gradients
Learning rate schedule | Performance scheduling or 1cycle


